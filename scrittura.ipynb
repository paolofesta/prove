{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrittura.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNx03XHBzGz5YnWOm5vdBx4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paolofesta/prove/blob/master/scrittura.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Scelta dell’oggetto di studio**\n",
        "La scelta di utilizzare Twitter, raccogliendo tweet da analizzare, è nata da una riflessione svolta con il relatore; dal momento che questo tipo di analisi non era ancora stata affrontata nel mio percorso di studi, l’interesse verso questo tipo di ricerca era elevato, proprio come l’impegno richiesto. Prima ancora di decidere il target della ricerca, era indispensabile conoscere i meccanismi di raccolta dei tweet, per capire cosa si potesse ottenere. In quest’ottica è stato fondamentale il Twitter Developer Portal, tramite esso è stato creato un developer account con accesso elevato, in quanto l’accesso standard comportava limiti eccessivi. Tramite la Dashboard del Developer Portal si poteva avere accesso ai propri progetti e alle proprie app, strumenti essenziali per ottenere accesso alle API di Twitter e quindi per raccogliere i tweet. Queste informazioni e molte altre sono consultabili nella sezione Docs del portale, che si è rivelata strumento utilissimo non solo in questa fase iniziale ma per tutta la durata del progetto. Una volta compreso cosa effettivamente si poteva raccogliere, la concentrazione si è focalizzata sulla scelta dell’oggetto di studio vero e proprio. La scelta è ricaduta sull’analisi dei tweet citanti le tre maggiori sigle sindacali italiane: Cgil, Cisl e Uil. I motivi sono molteplici: Il mutamento del ruolo dei sindacati e la crisi dei tassi di sindacalizzazione è stato un fenomeno riscontrato più volte durante il percorso di studi, suscitando molto interesse; inoltre, in questo modo è stato possibile raccogliere un buon numero di tweets, circa tredicimila, condizione essenziale per poter fare un’analisi valida. L’intento era quello di svolgere una Sentiment Analysis dei tweets raccolti, ovvero identificare sentimenti ed emozioni associati ai tweets, non fermandosi ai dati generali ma andando alla ricerca di variabili in grado di cambiare questa associazione. \n",
        "### **Scelta del linguaggio di programmazione**\n",
        "Le due opzioni prese in considerazione erano Python e R studio, entrambi i linguaggi di programmazione sono stati appresi durante il percorso di studi e sono da considerare come validi strumenti. La scelta è ricaduta su Python per diverse ragioni: innanzitutto, la maggiore diffusione, che porta ad una maggiore quantità di documentazione e ad una più ampia community di utilizzatori, molto utile per risolvere problemi e ricevere chiarimenti; inoltre, perché permette di utilizzare Google Colab, strumento molto comodo e versatile che permette di usare Python dal proprio browser. Per concludere, Python è stato indicato come primo nella top ten dei linguaggi di programmazione più popolari da imparare nel 2020 secondo la Northeastern Univeristy. \n",
        "\n",
        "### **Librerie utilizzate**\n",
        "NumPy è una libreria essenziale di Python, utilizzata per gestire le matrici numeriche (arrays); Pandas è invece una libreria costruita sulla base di quella precedente, che permette di gestire i dataframe, ovvero degli arrays multidimensionali in forma tabellare. Tweepy, permette di avere accesso alle API di twitter per raccogliere diverse tipologie di dati, esistono altre librerie simili, questa è però tra le più utilizzate, essendo indicata anche all’interno del Twitter Developer Portal. re e gensim sono librerie utilizzate per il topic modelling, cioè per processare il testo dei tweet prima di poter iniziare con la sentiment analisys. feel_it è una libreria tramite la quale si può utilizzare un modello che, attraverso un corpus di tweet classificati in base a quattro emozioni basiche(rabbia, paura, gioia, tristezza), permette di inferire sentimenti ed emozioni da tweet in lingua italiana. Infine, matplotlib.pyplot e seaborn, sono librerie utilizzate a fini di visualizzazione, per produrre word clouds e grafici.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lr9DOYL8faAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "import re\n",
        "import gensim\n",
        "import fell-it\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "KdgC5B-XT1lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Accesso alle API di Twitter**\n",
        "Tramite la seguente funzione, è stato possibile connettere l'ambiente di lavoro con il Developer account, inserendo le diverse password fornite dal Developer Portal; questo aspetto è stato indispensabile per poter interagire con le API di Twitter."
      ],
      "metadata": {
        "id": "grSqZNx0mNEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def connect_to_twitter_OAuth():\n",
        "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
        "\n",
        "    api = tweepy.API(auth, wait_on_rate_limit=True ,wait_on_rate_limit_notify=True)\n",
        "    return api\n",
        "    \n",
        "api = connect_to_twitter_OAuth()"
      ],
      "metadata": {
        "id": "VWwgj3AcnlGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scraping**\n",
        "Sono stati raccolti 13365 tweets nel cui testo venivano citati i profili delle tre maggiori sigle sindacali italiane: @cgilnazionale, @CislNazionale,@UILofficial. Durante questa fase è stata essenziale la consultazione del Developer Portal di Twitter, le limitazioni nella raccolta di tweet imposte sono infatti molto stringenti: impongono l'obbligo di poter raccogliere esclusivamente i tweet prodotti nei sette giorni precedenti alla richiesta, a meno che non si voglia raccogliere solo i tweet di uno specifico utente, in questo caso le limitazioni sono diverse. Inoltre, queste non si limitano solo alle sopracitate, esistono anche dei limiti riguardo al numero di tweet raccoglibili in un dato periodo di tempo, per questo è stato necessario utilizzare una funzione in grado di gestirle, riportata in seguito. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UjenSJ_Xk_aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_tweet_attributes(tweet_object):\n",
        "    tweet_list =[]\n",
        "    for tweet in tweet_object:\n",
        "        tweet_id = tweet.id \n",
        "        language= tweet.lang \n",
        "        name= tweet.user.name     \n",
        "        location= tweet.user.location\n",
        "        follower_count= tweet.user.followers_count\n",
        "        verified= tweet.user.verified\n",
        "        text = tweet.full_text \n",
        "        favorite_count = tweet.favorite_count\n",
        "        retweet_count = tweet.retweet_count\n",
        "        created_at = tweet.created_at \n",
        "        source = tweet.source \n",
        "       \n",
        "       \n",
        "        tweet_list.append({'tweet_id':tweet_id,\n",
        "                           'language':language,\n",
        "                          'text':text,\n",
        "                          'name':name,\n",
        "                          'location':location,\n",
        "                          'follower_count':follower_count,\n",
        "                          'verified': verified,\n",
        "                          'favorite_count':favorite_count,\n",
        "                          'retweet_count':retweet_count,\n",
        "                          'created_at':created_at, \n",
        "                          'source':source,})\n",
        "\n",
        "     \n",
        "    df1 = pd.DataFrame(tweet_list, columns=['tweet_id',\n",
        "                                           'text',\n",
        "                                           'language',\n",
        "                                           'name',\n",
        "                                           'follower_count',\n",
        "                                           'verified',\n",
        "                                           'location',    \n",
        "                                           'favorite_count',\n",
        "                                           'retweet_count',\n",
        "                                           'created_at',\n",
        "                                           'source'])\n",
        "    return df1\n",
        "\n",
        "\n",
        "df1 = extract_tweet_attributes(pp1)\n"
      ],
      "metadata": {
        "id": "ujBTSGucH__y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il numero di tweet si è poi ridotto a 12560 eliminando i tweet incompatibili con la ricerca. La raccolta è avvenuta in due momenti: in data 12/5/2022 sono stati raccolti i tweet prodotti dal 6/5 al 12/5, In data 23/7/2022 sono stati raccolti i tweet prodotti dal 17/7 al 23/7, per via dei suddetti limiti. Per ogni tweet sono state raccolte le seguenti variabili: \n",
        "*   L’ID del tweet\n",
        "*   La lingua in cui è stato scritto\n",
        "*   Il testo del tweet\n",
        "*   Il nome dell’utente che l’ha prodotto\n",
        "*   La location( qualora indicata)\n",
        "*   Il numero di follower dell’utente \n",
        "*   Lo status di verificato o meno dell’utente \n",
        "*   Il numero di favourites del tweet\n",
        "*   Il numero di retweet\n",
        "*   L’orario in cui è stato creato \n",
        "*   La fonte\n"
      ],
      "metadata": {
        "id": "3WU6nAKAp8Kb"
      }
    }
  ]
}